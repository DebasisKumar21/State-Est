{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18db78ea-68f8-499b-9e11-0867dd63cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import torch \n",
    "import gpytorch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc66497-8a89-4bae-ad32-318d79dfa175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27def809-eade-4e92-873f-bdab1631ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4397, 18]) torch.Size([4397])\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_data = np.load(\"./Dataset/train_no_pos.npy\")\n",
    "val_data = np.load(\"./Dataset/val_no_pos.npy\")\n",
    "test_data = np.load(\"./Dataset/test_no_pos.npy\")\n",
    "\n",
    "train_x = torch.from_numpy(train_data[:, :-1]) #.to(device)\n",
    "# train_x = train_x.view(-1, 18, 1)\n",
    "\n",
    "train_y = torch.from_numpy(train_data[:, -1]) #.to(device)\n",
    "# train_y = train_y.view(-1, 1)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff4e515-9061-4e0e-ac77-c43b032ab18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact GPs\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Small MLP Feature Extractor\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.add_module(\"linear1\", nn.Linear(input_dim, 256))\n",
    "        self.add_module(\"relu1\", nn.ReLU())\n",
    "        self.add_module(\"b1\", nn.BatchNorm1d(256))\n",
    "        self.add_module(\"linear2\", nn.Linear(256, 128))\n",
    "        self.add_module(\"relu2\", nn.ReLU())\n",
    "        self.add_module(\"linear3\", nn.Linear(128, output_dim))\n",
    "        self.add_module(\"dropout\", nn.Dropout(p=0.25))\n",
    "    \n",
    "# MLP\n",
    "feature_extractor = MLP(input_dim=18, output_dim=2)\n",
    "\n",
    "# MLP - GP\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            \n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# model = ExactGPModel(train_x, train_y, likelihood)\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1317da-44bd-4560-b590-b13c9aac63a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/240 - Loss: nan time: 0.743\n",
      "Iter 2/240 - Loss: nan time: 0.059\n",
      "Iter 3/240 - Loss: 0.687 time: 0.059\n",
      "Iter 4/240 - Loss: 0.649 time: 0.055\n",
      "Iter 5/240 - Loss: 0.612 time: 0.055\n",
      "Iter 6/240 - Loss: 0.574 time: 0.053\n",
      "Iter 7/240 - Loss: 0.536 time: 0.054\n",
      "Iter 8/240 - Loss: 0.498 time: 0.057\n",
      "Iter 9/240 - Loss: 0.458 time: 0.058\n",
      "Iter 10/240 - Loss: 0.418 time: 0.057\n",
      "Iter 11/240 - Loss: 0.379 time: 0.056\n",
      "Iter 12/240 - Loss: 0.338 time: 0.053\n",
      "Iter 13/240 - Loss: 0.297 time: 0.055\n",
      "Iter 14/240 - Loss: 0.256 time: 0.057\n",
      "Iter 15/240 - Loss: 0.215 time: 0.057\n",
      "Iter 16/240 - Loss: 0.174 time: 0.058\n",
      "Iter 17/240 - Loss: 0.135 time: 0.057\n",
      "Iter 18/240 - Loss: 0.092 time: 0.053\n",
      "Iter 19/240 - Loss: 0.053 time: 0.052\n",
      "Iter 20/240 - Loss: 0.015 time: 0.052\n",
      "Iter 21/240 - Loss: -0.025 time: 0.054\n",
      "Iter 22/240 - Loss: -0.064 time: 0.054\n",
      "Iter 23/240 - Loss: -0.102 time: 0.053\n",
      "Iter 24/240 - Loss: -0.140 time: 0.056\n",
      "Iter 25/240 - Loss: -0.176 time: 0.058\n",
      "Iter 26/240 - Loss: -0.209 time: 0.057\n",
      "Iter 27/240 - Loss: -0.243 time: 0.057\n",
      "Iter 28/240 - Loss: -0.276 time: 0.055\n",
      "Iter 29/240 - Loss: -0.308 time: 0.054\n",
      "Iter 30/240 - Loss: -0.337 time: 0.054\n",
      "Iter 31/240 - Loss: -0.364 time: 0.053\n",
      "Iter 32/240 - Loss: -0.385 time: 0.053\n",
      "Iter 33/240 - Loss: -0.410 time: 0.057\n",
      "Iter 34/240 - Loss: -0.433 time: 0.059\n",
      "Iter 35/240 - Loss: -0.453 time: 0.057\n",
      "Iter 36/240 - Loss: -0.468 time: 0.058\n",
      "Iter 37/240 - Loss: -0.481 time: 0.058\n",
      "Iter 38/240 - Loss: -0.495 time: 0.054\n",
      "Iter 39/240 - Loss: -0.504 time: 0.053\n",
      "Iter 40/240 - Loss: -0.504 time: 0.054\n",
      "Iter 41/240 - Loss: -0.518 time: 0.054\n",
      "Iter 42/240 - Loss: -0.518 time: 0.054\n",
      "Iter 43/240 - Loss: -0.523 time: 0.054\n",
      "Iter 44/240 - Loss: -0.532 time: 0.053\n",
      "Iter 45/240 - Loss: -0.528 time: 0.055\n",
      "Iter 46/240 - Loss: -0.528 time: 0.058\n",
      "Iter 47/240 - Loss: -0.526 time: 0.057\n",
      "Iter 48/240 - Loss: -0.522 time: 0.058\n",
      "Iter 49/240 - Loss: -0.527 time: 0.058\n",
      "Iter 50/240 - Loss: -0.531 time: 0.055\n",
      "Iter 51/240 - Loss: -0.522 time: 0.053\n",
      "Iter 52/240 - Loss: -0.523 time: 0.053\n",
      "Iter 53/240 - Loss: -0.497 time: 0.054\n",
      "Iter 54/240 - Loss: nan time: 0.053\n",
      "Iter 55/240 - Loss: -0.495 time: 0.053\n",
      "Iter 56/240 - Loss: -0.513 time: 0.053\n",
      "Iter 57/240 - Loss: -0.520 time: 0.055\n",
      "Iter 58/240 - Loss: -0.491 time: 0.058\n",
      "Iter 59/240 - Loss: -0.496 time: 0.057\n",
      "Iter 60/240 - Loss: -0.510 time: 0.058\n",
      "Iter 61/240 - Loss: -0.506 time: 0.059\n",
      "Iter 62/240 - Loss: -0.503 time: 0.058\n",
      "Iter 63/240 - Loss: -0.496 time: 0.054\n",
      "Iter 64/240 - Loss: -0.473 time: 0.053\n",
      "Iter 65/240 - Loss: -0.504 time: 0.055\n",
      "Iter 66/240 - Loss: -0.492 time: 0.055\n",
      "Iter 67/240 - Loss: -0.480 time: 0.054\n",
      "Iter 68/240 - Loss: -0.476 time: 0.054\n",
      "Iter 69/240 - Loss: -0.485 time: 0.055\n",
      "Iter 70/240 - Loss: -0.475 time: 0.055\n",
      "Iter 71/240 - Loss: -0.491 time: 0.059\n",
      "Iter 72/240 - Loss: -0.480 time: 0.058\n",
      "Iter 73/240 - Loss: -0.487 time: 0.059\n",
      "Iter 74/240 - Loss: -0.493 time: 0.058\n",
      "Iter 75/240 - Loss: -0.487 time: 0.055\n",
      "Iter 76/240 - Loss: -0.498 time: 0.054\n",
      "Iter 77/240 - Loss: -0.503 time: 0.056\n",
      "Iter 78/240 - Loss: -0.500 time: 0.053\n",
      "Iter 79/240 - Loss: -0.505 time: 0.058\n",
      "Iter 80/240 - Loss: -0.517 time: 0.059\n",
      "Iter 81/240 - Loss: -0.511 time: 0.058\n",
      "Iter 82/240 - Loss: -0.531 time: 0.058\n",
      "Iter 83/240 - Loss: -0.510 time: 0.058\n",
      "Iter 84/240 - Loss: -0.527 time: 0.054\n",
      "Iter 85/240 - Loss: -0.521 time: 0.055\n",
      "Iter 86/240 - Loss: -0.517 time: 0.054\n",
      "Iter 87/240 - Loss: -0.525 time: 0.054\n",
      "Iter 88/240 - Loss: -0.525 time: 0.054\n",
      "Iter 89/240 - Loss: -0.535 time: 0.055\n",
      "Iter 90/240 - Loss: -0.534 time: 0.058\n",
      "Iter 91/240 - Loss: -0.537 time: 0.057\n",
      "Iter 92/240 - Loss: -0.545 time: 0.058\n",
      "Iter 93/240 - Loss: -0.543 time: 0.058\n",
      "Iter 94/240 - Loss: -0.544 time: 0.055\n",
      "Iter 95/240 - Loss: -0.546 time: 0.054\n",
      "Iter 96/240 - Loss: -0.557 time: 0.054\n",
      "Iter 97/240 - Loss: -0.547 time: 0.054\n",
      "Iter 98/240 - Loss: -0.545 time: 0.055\n",
      "Iter 99/240 - Loss: -0.532 time: 0.058\n",
      "Iter 100/240 - Loss: -0.512 time: 0.058\n",
      "Iter 101/240 - Loss: -0.514 time: 0.058\n",
      "Iter 102/240 - Loss: -0.533 time: 0.057\n",
      "Iter 103/240 - Loss: -0.544 time: 0.053\n",
      "Iter 104/240 - Loss: -0.529 time: 0.053\n",
      "Iter 105/240 - Loss: -0.528 time: 0.055\n",
      "Iter 106/240 - Loss: -0.534 time: 0.054\n",
      "Iter 107/240 - Loss: -0.529 time: 0.053\n",
      "Iter 108/240 - Loss: -0.529 time: 0.053\n",
      "Iter 109/240 - Loss: -0.528 time: 0.055\n",
      "Iter 110/240 - Loss: -0.536 time: 0.054\n",
      "Iter 111/240 - Loss: -0.539 time: 0.053\n",
      "Iter 112/240 - Loss: -0.538 time: 0.058\n",
      "Iter 113/240 - Loss: -0.534 time: 0.059\n",
      "Iter 114/240 - Loss: -0.555 time: 0.058\n",
      "Iter 115/240 - Loss: -0.555 time: 0.057\n",
      "Iter 116/240 - Loss: -0.556 time: 0.056\n",
      "Iter 117/240 - Loss: -0.547 time: 0.055\n",
      "Iter 118/240 - Loss: -0.568 time: 0.054\n",
      "Iter 119/240 - Loss: -0.551 time: 0.053\n",
      "Iter 120/240 - Loss: -0.554 time: 0.053\n",
      "Iter 121/240 - Loss: -0.573 time: 0.055\n",
      "Iter 122/240 - Loss: -0.561 time: 0.054\n",
      "Iter 123/240 - Loss: -0.572 time: 0.056\n",
      "Iter 124/240 - Loss: -0.565 time: 0.058\n",
      "Iter 125/240 - Loss: -0.570 time: 0.058\n",
      "Iter 126/240 - Loss: -0.562 time: 0.057\n",
      "Iter 127/240 - Loss: -0.564 time: 0.058\n",
      "Iter 128/240 - Loss: -0.570 time: 0.054\n",
      "Iter 129/240 - Loss: -0.569 time: 0.055\n",
      "Iter 130/240 - Loss: -0.580 time: 0.053\n",
      "Iter 131/240 - Loss: -0.566 time: 0.053\n",
      "Iter 132/240 - Loss: -0.578 time: 0.053\n",
      "Iter 133/240 - Loss: -0.573 time: 0.054\n",
      "Iter 134/240 - Loss: -0.581 time: 0.054\n",
      "Iter 135/240 - Loss: -0.577 time: 0.057\n",
      "Iter 136/240 - Loss: -0.566 time: 0.057\n",
      "Iter 137/240 - Loss: -0.562 time: 0.058\n",
      "Iter 138/240 - Loss: -0.586 time: 0.057\n",
      "Iter 139/240 - Loss: -0.573 time: 0.055\n",
      "Iter 140/240 - Loss: -0.553 time: 0.054\n",
      "Iter 141/240 - Loss: -0.550 time: 0.054\n",
      "Iter 142/240 - Loss: -0.546 time: 0.054\n",
      "Iter 143/240 - Loss: -0.552 time: 0.053\n",
      "Iter 144/240 - Loss: -0.550 time: 0.054\n",
      "Iter 145/240 - Loss: -0.568 time: 0.054\n",
      "Iter 146/240 - Loss: -0.568 time: 0.054\n",
      "Iter 147/240 - Loss: -0.582 time: 0.057\n",
      "Iter 148/240 - Loss: -0.565 time: 0.059\n",
      "Iter 149/240 - Loss: -0.579 time: 0.059\n",
      "Iter 150/240 - Loss: -0.585 time: 0.058\n",
      "Iter 151/240 - Loss: -0.568 time: 0.058\n",
      "Iter 152/240 - Loss: -0.575 time: 0.056\n",
      "Iter 153/240 - Loss: -0.584 time: 0.055\n",
      "Iter 154/240 - Loss: -0.582 time: 0.054\n",
      "Iter 155/240 - Loss: -0.578 time: 0.053\n",
      "Iter 156/240 - Loss: -0.582 time: 0.053\n",
      "Iter 157/240 - Loss: -0.574 time: 0.054\n",
      "Iter 158/240 - Loss: -0.581 time: 0.055\n",
      "Iter 159/240 - Loss: -0.595 time: 0.053\n",
      "Iter 160/240 - Loss: -0.584 time: 0.058\n",
      "Iter 161/240 - Loss: -0.580 time: 0.059\n",
      "Iter 162/240 - Loss: -0.590 time: 0.058\n",
      "Iter 163/240 - Loss: -0.584 time: 0.058\n",
      "Iter 164/240 - Loss: -0.580 time: 0.055\n",
      "Iter 165/240 - Loss: -0.586 time: 0.055\n",
      "Iter 166/240 - Loss: -0.592 time: 0.054\n",
      "Iter 167/240 - Loss: -0.582 time: 0.054\n",
      "Iter 168/240 - Loss: -0.591 time: 0.054\n",
      "Iter 169/240 - Loss: -0.587 time: 0.055\n",
      "Iter 170/240 - Loss: -0.582 time: 0.054\n",
      "Iter 171/240 - Loss: -0.589 time: 0.054\n",
      "Iter 172/240 - Loss: -0.588 time: 0.055\n",
      "Iter 173/240 - Loss: -0.594 time: 0.060\n",
      "Iter 174/240 - Loss: -0.590 time: 0.058\n",
      "Iter 175/240 - Loss: -0.610 time: 0.057\n",
      "Iter 176/240 - Loss: -0.586 time: 0.057\n",
      "Iter 177/240 - Loss: -0.592 time: 0.056\n",
      "Iter 178/240 - Loss: -0.592 time: 0.053\n",
      "Iter 179/240 - Loss: -0.589 time: 0.053\n",
      "Iter 180/240 - Loss: -0.580 time: 0.054\n",
      "Iter 181/240 - Loss: -0.593 time: 0.055\n",
      "Iter 182/240 - Loss: -0.572 time: 0.054\n",
      "Iter 183/240 - Loss: -0.560 time: 0.054\n",
      "Iter 184/240 - Loss: -0.570 time: 0.054\n",
      "Iter 185/240 - Loss: -0.580 time: 0.055\n",
      "Iter 186/240 - Loss: -0.571 time: 0.055\n",
      "Iter 187/240 - Loss: -0.562 time: 0.053\n",
      "Iter 188/240 - Loss: -0.571 time: 0.059\n",
      "Iter 189/240 - Loss: -0.567 time: 0.059\n",
      "Iter 190/240 - Loss: -0.573 time: 0.061\n",
      "Iter 191/240 - Loss: -0.580 time: 0.058\n",
      "Iter 192/240 - Loss: -0.576 time: 0.058\n",
      "Iter 193/240 - Loss: -0.591 time: 0.057\n",
      "Iter 194/240 - Loss: -0.587 time: 0.054\n",
      "Iter 195/240 - Loss: -0.575 time: 0.054\n",
      "Iter 196/240 - Loss: -0.571 time: 0.054\n",
      "Iter 197/240 - Loss: -0.578 time: 0.055\n",
      "Iter 198/240 - Loss: -0.563 time: 0.055\n",
      "Iter 199/240 - Loss: -0.554 time: 0.054\n",
      "Iter 200/240 - Loss: -0.557 time: 0.054\n",
      "Iter 201/240 - Loss: -0.558 time: 0.055\n",
      "Iter 202/240 - Loss: -0.563 time: 0.058\n",
      "Iter 203/240 - Loss: -0.566 time: 0.058\n",
      "Iter 204/240 - Loss: -0.547 time: 0.058\n",
      "Iter 205/240 - Loss: -0.554 time: 0.059\n",
      "Iter 206/240 - Loss: -0.546 time: 0.056\n",
      "Iter 207/240 - Loss: -0.552 time: 0.054\n",
      "Iter 208/240 - Loss: -0.565 time: 0.054\n",
      "Iter 209/240 - Loss: -0.560 time: 0.055\n",
      "Iter 210/240 - Loss: -0.554 time: 0.055\n",
      "Iter 211/240 - Loss: -0.562 time: 0.054\n",
      "Iter 212/240 - Loss: -0.557 time: 0.054\n",
      "Iter 213/240 - Loss: -0.575 time: 0.055\n",
      "Iter 214/240 - Loss: -0.575 time: 0.054\n",
      "Iter 215/240 - Loss: -0.570 time: 0.057\n",
      "Iter 216/240 - Loss: -0.578 time: 0.058\n",
      "Iter 217/240 - Loss: -0.586 time: 0.059\n",
      "Iter 218/240 - Loss: -0.574 time: 0.058\n",
      "Iter 219/240 - Loss: -0.578 time: 0.057\n",
      "Iter 220/240 - Loss: -0.573 time: 0.054\n",
      "Iter 221/240 - Loss: -0.582 time: 0.056\n",
      "Iter 222/240 - Loss: -0.588 time: 0.054\n",
      "Iter 223/240 - Loss: -0.585 time: 0.053\n",
      "Iter 224/240 - Loss: -0.591 time: 0.053\n",
      "Iter 225/240 - Loss: -0.582 time: 0.054\n",
      "Iter 226/240 - Loss: -0.582 time: 0.054\n",
      "Iter 227/240 - Loss: -0.586 time: 0.053\n",
      "Iter 228/240 - Loss: -0.571 time: 0.055\n",
      "Iter 229/240 - Loss: -0.581 time: 0.059\n",
      "Iter 230/240 - Loss: -0.581 time: 0.058\n",
      "Iter 231/240 - Loss: -0.583 time: 0.057\n",
      "Iter 232/240 - Loss: -0.601 time: 0.058\n",
      "Iter 233/240 - Loss: -0.584 time: 0.058\n",
      "Iter 234/240 - Loss: -0.584 time: 0.054\n",
      "Iter 235/240 - Loss: -0.597 time: 0.053\n",
      "Iter 236/240 - Loss: -0.605 time: 0.054\n",
      "Iter 237/240 - Loss: -0.580 time: 0.055\n",
      "Iter 238/240 - Loss: -0.588 time: 0.054\n",
      "Iter 239/240 - Loss: -0.586 time: 0.053\n",
      "Iter 240/240 - Loss: -0.582 time: 0.053\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using GPU\n",
    "model = model.to(device)\n",
    "likelihood = likelihood.to(device)\n",
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # ExactGP\n",
    "\n",
    "# MLP GP\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 240\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    \n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    \n",
    "    print('Iter %d/%d - Loss: %.3f time: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        (time.time() - start_time)\n",
    "    ))\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7a77c5-87fe-4458-a450-1fb04d44bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([489, 18]) torch.Size([489])\n"
     ]
    }
   ],
   "source": [
    "val_x = torch.from_numpy(val_data[:, :-1]).to(device)\n",
    "# val_x = val_x.view(-1, 18, 1)\n",
    "\n",
    "val_y = torch.from_numpy(val_data[:, -1]).to(device)\n",
    "# val_y = val_y.view(-1, 1)\n",
    "\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2ce784-9842-496f-b507-5a3f80f8bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.146\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(val_x))\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "\n",
    "# RMSE\n",
    "rmse = torch.sqrt(torch.mean(torch.pow(observed_pred.mean - val_y, 2)))\n",
    "print(f\"RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b82d0400-9660-4dbe-9759-50e2ee2566ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4397, 18]),\n",
       " torch.Size([4397]),\n",
       " torch.Size([489, 18]),\n",
       " torch.Size([489]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = mean.cpu()\n",
    "lower = lower.cpu()\n",
    "upper = upper.cpu()\n",
    "\n",
    "train_x = train_x.cpu()\n",
    "train_y = train_y.cpu()\n",
    "val_x = val_x.cpu()\n",
    "val_y = val_y.cpu()\n",
    "\n",
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61ef493-a4cc-4f65-965c-07afbeba5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(18, 12))\n",
    "    X = np.linspace(0, mean.shape[0], mean.shape[0])*0.05\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(X, val_y.numpy())\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(X, mean.numpy())\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(X, lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_xlim([10, 15])\n",
    "    # ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
