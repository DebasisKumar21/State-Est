{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e029b25-21c2-4ea0-b21c-999754cb0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f042fee-abc0-4954-acd7-c2d7965b1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b17c26-140d-47c2-8048-d49008fe20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN\n",
    "class ToyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ToyNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.linear1(x))\n",
    "        out = self.activation(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "    \n",
    "# Expressive LSTM v2\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size) # nn.Linear(hidden_size * 2, hidden_size) for self lstm_out\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # h0 = torch.zeros((4, batch_size, 256), device=device, requires_grad=True)\n",
    "        # c0 = torch.zeros((4, batch_size, 256), device=device, requires_grad=True)\n",
    "        out, (hn, cn) = self.lstm(x.view(batch_size, 1, -1))\n",
    "        out = self.activation(self.linear1(hn[0]))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b24ba1-9d8f-4b64-ace4-42d2cea13020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27378, 8),\n",
       " (27378, 1),\n",
       " torch.Size([25000, 8]),\n",
       " torch.Size([25000, 1]),\n",
       " torch.Size([500, 8]),\n",
       " torch.Size([500, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sio.loadmat(\"./Dataset/data_23_03.mat\")[\"training_matrix\"]\n",
    "\n",
    "X = data[:, :-1].astype(np.float32)\n",
    "y = data[:, -1].reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# Normalization\n",
    "mu = X.mean()\n",
    "sigma = X.std()\n",
    "\n",
    "# Normalized Dataset\n",
    "X_norm = (X - mu) / sigma\n",
    "\n",
    "# Inputs & Targets\n",
    "X_train = torch.from_numpy(X_norm[0:25_000]).to(device)\n",
    "y_train = torch.from_numpy(y[0:25_000]).to(device)\n",
    "\n",
    "# Inputs & Targets\n",
    "X_test = torch.from_numpy(X_norm[25_000:25_500]).to(device)\n",
    "y_test = torch.from_numpy(y[25_000:25_500]).to(device)\n",
    "\n",
    "X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d9d80-aa9d-44bc-bc21-37ffd58d5581",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ToyNet Inputs\n",
    "input_size = 2\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "# FNN\n",
    "model = ToyNet(input_size, hidden_size, output_size).to(device)\n",
    "delta_t = torch.tensor([0.05], device=device)\n",
    "\n",
    "epochs = 150\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 6e-6)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "avg_train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    losses_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        \n",
    "        inp = X_train[i].view(4, -1)\n",
    "        pred = torch.cumsum(model(inp) * delta_t, dim=0)\n",
    "\n",
    "        loss = loss_fn(pred[-1], y_train[i])        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()        \n",
    "        losses_train.append(loss.detach().cpu().numpy())\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {np.average(losses_train)}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_train_loss.append(np.average(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20164e-0fcf-4b6c-a9b3-82256e6f4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[12].view(4, -1)\n",
    "\n",
    "pred = model(x)\n",
    "\n",
    "print(pred)\n",
    "print(y_test[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a624ca8-0a60-450f-b8a1-9c07b551105b",
   "metadata": {},
   "source": [
    "### Autoregressive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee1437-909a-4b9d-a9a1-2a48c2210e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive RNN\n",
    "class ToyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ToyRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden))\n",
    "        hidden = self.activation(self.i2h(combined))\n",
    "        output = self.activation(self.i2h(combined))\n",
    "        output = self.h2o(output)\n",
    "        return output, hidden \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size, device=device)\n",
    "\n",
    "# ToyRNN Inputs\n",
    "input_size = 2 \n",
    "hidden_size = 512\n",
    "output_size = 1\n",
    "\n",
    "# Autoregressive Model\n",
    "model = ToyRNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 6e-6)\n",
    "delta_t = torch.tensor([0.05], device=device)\n",
    "\n",
    "epochs = 240\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "avg_train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    losses_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        l, k = 0, 2\n",
    "        hidden = model.initHidden()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for j in range(4):\n",
    "            inp = X_train[i, l:k]\n",
    "            output, hidden = model(inp, hidden)\n",
    "            l += 2\n",
    "            k += 2\n",
    "\n",
    "        loss = loss_fn(output, y_train[i])        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_train.append(loss.detach().cpu().numpy())\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {np.average(losses_train)}\")\n",
    "    avg_train_loss.append(np.average(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bdc84-8a99-48fd-b088-9d43632113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    hidden = model.initHidden()\n",
    "    \n",
    "    l, k = 0, 2\n",
    "    for j in range(4):\n",
    "        inp = X_test[0, l:k]\n",
    "        output, hidden = model(inp, hidden)\n",
    "        l += 2\n",
    "        k += 2\n",
    "        \n",
    "        print(f\"For x{j+1}, Pred y{j+1}: {output.item()}\\n\")\n",
    "        \n",
    "\n",
    "    print(f\"True y: {y_test[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305fdd30-cbf6-4e99-852c-430be846e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.2773677706718445\n",
      "Epoch: 2, Train Loss: 0.19978561997413635\n",
      "Epoch: 3, Train Loss: 0.17691145837306976\n",
      "Epoch: 4, Train Loss: 0.168458491563797\n",
      "Epoch: 5, Train Loss: 0.1645454317331314\n",
      "Epoch: 6, Train Loss: 0.16199879348278046\n",
      "Epoch: 7, Train Loss: 0.16009503602981567\n",
      "Epoch: 8, Train Loss: 0.15861491858959198\n",
      "Epoch: 9, Train Loss: 0.15741877257823944\n",
      "Epoch: 10, Train Loss: 0.15644396841526031\n",
      "Epoch: 11, Train Loss: 0.15657185018062592\n",
      "Epoch: 12, Train Loss: 0.15460681915283203\n",
      "Epoch: 13, Train Loss: 0.15425047278404236\n",
      "Epoch: 14, Train Loss: 0.15411032736301422\n",
      "Epoch: 15, Train Loss: 0.1540076732635498\n",
      "Epoch: 16, Train Loss: 0.15391629934310913\n",
      "Epoch: 17, Train Loss: 0.15382935106754303\n",
      "Epoch: 18, Train Loss: 0.15374493598937988\n",
      "Epoch: 19, Train Loss: 0.15366394817829132\n",
      "Epoch: 20, Train Loss: 0.15358316898345947\n",
      "Epoch: 21, Train Loss: 0.153359055519104\n",
      "Epoch: 22, Train Loss: 0.1533491164445877\n",
      "Epoch: 23, Train Loss: 0.15334057807922363\n",
      "Epoch: 24, Train Loss: 0.15333214402198792\n",
      "Epoch: 25, Train Loss: 0.15332268178462982\n",
      "Epoch: 26, Train Loss: 0.1533140391111374\n",
      "Epoch: 27, Train Loss: 0.15330521762371063\n",
      "Epoch: 28, Train Loss: 0.15329553186893463\n",
      "Epoch: 29, Train Loss: 0.1532870978116989\n",
      "Epoch: 30, Train Loss: 0.15327803790569305\n",
      "Epoch: 31, Train Loss: 0.15325291454792023\n",
      "Epoch: 32, Train Loss: 0.15325190126895905\n",
      "Epoch: 33, Train Loss: 0.153251051902771\n",
      "Epoch: 34, Train Loss: 0.15324929356575012\n",
      "Epoch: 35, Train Loss: 0.15324841439723969\n",
      "Epoch: 36, Train Loss: 0.15324699878692627\n",
      "Epoch: 37, Train Loss: 0.15324611961841583\n",
      "Epoch: 38, Train Loss: 0.15324503183364868\n",
      "Epoch: 39, Train Loss: 0.15324345231056213\n",
      "Epoch: 40, Train Loss: 0.1532428115606308\n",
      "Epoch: 41, Train Loss: 0.15324047207832336\n",
      "Epoch: 42, Train Loss: 0.15324032306671143\n",
      "Epoch: 43, Train Loss: 0.15324008464813232\n",
      "Epoch: 44, Train Loss: 0.15324001014232635\n",
      "Epoch: 45, Train Loss: 0.1532396525144577\n",
      "Epoch: 46, Train Loss: 0.15323954820632935\n",
      "Epoch: 47, Train Loss: 0.15323960781097412\n",
      "Epoch: 48, Train Loss: 0.15323936939239502\n",
      "Epoch: 49, Train Loss: 0.15323905646800995\n",
      "Epoch: 50, Train Loss: 0.15323922038078308\n",
      "Epoch: 51, Train Loss: 0.15323872864246368\n",
      "Epoch: 52, Train Loss: 0.15323872864246368\n",
      "Epoch: 53, Train Loss: 0.15323874354362488\n",
      "Epoch: 54, Train Loss: 0.15323874354362488\n",
      "Epoch: 55, Train Loss: 0.15323872864246368\n",
      "Epoch: 56, Train Loss: 0.15323872864246368\n",
      "Epoch: 57, Train Loss: 0.15323875844478607\n",
      "Epoch: 58, Train Loss: 0.15323877334594727\n",
      "Epoch: 59, Train Loss: 0.15323877334594727\n",
      "Epoch: 60, Train Loss: 0.15323880314826965\n",
      "Epoch: 61, Train Loss: 0.15323880314826965\n",
      "Epoch: 62, Train Loss: 0.15323880314826965\n",
      "Epoch: 63, Train Loss: 0.15323880314826965\n",
      "Epoch: 64, Train Loss: 0.15323880314826965\n",
      "Epoch: 65, Train Loss: 0.15323880314826965\n",
      "Epoch: 66, Train Loss: 0.15323880314826965\n",
      "Epoch: 67, Train Loss: 0.15323880314826965\n",
      "Epoch: 68, Train Loss: 0.15323880314826965\n",
      "Epoch: 69, Train Loss: 0.15323880314826965\n",
      "Epoch: 70, Train Loss: 0.15323880314826965\n",
      "Epoch: 71, Train Loss: 0.15323880314826965\n",
      "Epoch: 72, Train Loss: 0.15323880314826965\n",
      "Epoch: 73, Train Loss: 0.15323880314826965\n",
      "Epoch: 74, Train Loss: 0.15323880314826965\n",
      "Epoch: 75, Train Loss: 0.15323880314826965\n",
      "Epoch: 76, Train Loss: 0.15323880314826965\n",
      "Epoch: 77, Train Loss: 0.15323880314826965\n",
      "Epoch: 78, Train Loss: 0.15323880314826965\n",
      "Epoch: 79, Train Loss: 0.15323880314826965\n",
      "Epoch: 80, Train Loss: 0.15323880314826965\n",
      "Epoch: 81, Train Loss: 0.15323880314826965\n",
      "Epoch: 82, Train Loss: 0.15323880314826965\n",
      "Epoch: 83, Train Loss: 0.15323880314826965\n",
      "Epoch: 84, Train Loss: 0.15323880314826965\n",
      "Epoch: 85, Train Loss: 0.15323880314826965\n",
      "Epoch: 86, Train Loss: 0.15323880314826965\n",
      "Epoch: 87, Train Loss: 0.15323880314826965\n",
      "Epoch: 88, Train Loss: 0.15323880314826965\n",
      "Epoch: 89, Train Loss: 0.15323880314826965\n",
      "Epoch: 90, Train Loss: 0.15323880314826965\n",
      "Epoch: 91, Train Loss: 0.15323880314826965\n",
      "Epoch: 92, Train Loss: 0.15323880314826965\n",
      "Epoch: 93, Train Loss: 0.15323880314826965\n",
      "Epoch: 94, Train Loss: 0.15323880314826965\n",
      "Epoch: 95, Train Loss: 0.15323880314826965\n",
      "Epoch: 96, Train Loss: 0.15323880314826965\n",
      "Epoch: 97, Train Loss: 0.15323880314826965\n",
      "Epoch: 98, Train Loss: 0.15323880314826965\n",
      "Epoch: 99, Train Loss: 0.15323880314826965\n",
      "Epoch: 100, Train Loss: 0.15323880314826965\n",
      "Epoch: 101, Train Loss: 0.15323880314826965\n",
      "Epoch: 102, Train Loss: 0.15323880314826965\n",
      "Epoch: 103, Train Loss: 0.15323880314826965\n",
      "Epoch: 104, Train Loss: 0.15323880314826965\n",
      "Epoch: 105, Train Loss: 0.15323880314826965\n",
      "Epoch: 106, Train Loss: 0.15323880314826965\n",
      "Epoch: 107, Train Loss: 0.15323880314826965\n",
      "Epoch: 108, Train Loss: 0.15323880314826965\n",
      "Epoch: 109, Train Loss: 0.15323880314826965\n",
      "Epoch: 110, Train Loss: 0.15323880314826965\n",
      "Epoch: 111, Train Loss: 0.15323880314826965\n",
      "Epoch: 112, Train Loss: 0.15323880314826965\n",
      "Epoch: 113, Train Loss: 0.15323880314826965\n",
      "Epoch: 114, Train Loss: 0.15323880314826965\n",
      "Epoch: 115, Train Loss: 0.15323880314826965\n",
      "Epoch: 116, Train Loss: 0.15323880314826965\n",
      "Epoch: 117, Train Loss: 0.15323880314826965\n",
      "Epoch: 118, Train Loss: 0.15323880314826965\n",
      "Epoch: 119, Train Loss: 0.15323880314826965\n",
      "Epoch: 120, Train Loss: 0.15323880314826965\n",
      "Epoch: 121, Train Loss: 0.15323880314826965\n",
      "Epoch: 122, Train Loss: 0.15323880314826965\n",
      "Epoch: 123, Train Loss: 0.15323880314826965\n",
      "Epoch: 124, Train Loss: 0.15323880314826965\n",
      "Epoch: 125, Train Loss: 0.15323880314826965\n",
      "Epoch: 126, Train Loss: 0.15323880314826965\n",
      "Epoch: 127, Train Loss: 0.15323880314826965\n",
      "Epoch: 128, Train Loss: 0.15323880314826965\n",
      "Epoch: 129, Train Loss: 0.15323880314826965\n",
      "Epoch: 130, Train Loss: 0.15323880314826965\n",
      "Epoch: 131, Train Loss: 0.15323880314826965\n",
      "Epoch: 132, Train Loss: 0.15323880314826965\n",
      "Epoch: 133, Train Loss: 0.15323880314826965\n",
      "Epoch: 134, Train Loss: 0.15323880314826965\n",
      "Epoch: 135, Train Loss: 0.15323880314826965\n",
      "Epoch: 136, Train Loss: 0.15323880314826965\n",
      "Epoch: 137, Train Loss: 0.15323880314826965\n",
      "Epoch: 138, Train Loss: 0.15323880314826965\n",
      "Epoch: 139, Train Loss: 0.15323880314826965\n",
      "Epoch: 140, Train Loss: 0.15323880314826965\n",
      "Epoch: 141, Train Loss: 0.15323880314826965\n",
      "Epoch: 142, Train Loss: 0.15323880314826965\n",
      "Epoch: 143, Train Loss: 0.15323880314826965\n",
      "Epoch: 144, Train Loss: 0.15323880314826965\n",
      "Epoch: 145, Train Loss: 0.15323880314826965\n",
      "Epoch: 146, Train Loss: 0.15323880314826965\n",
      "Epoch: 147, Train Loss: 0.15323880314826965\n",
      "Epoch: 148, Train Loss: 0.15323880314826965\n",
      "Epoch: 149, Train Loss: 0.15323880314826965\n",
      "Epoch: 150, Train Loss: 0.15323880314826965\n",
      "Epoch: 151, Train Loss: 0.15323880314826965\n",
      "Epoch: 152, Train Loss: 0.15323880314826965\n",
      "Epoch: 153, Train Loss: 0.15323880314826965\n",
      "Epoch: 154, Train Loss: 0.15323880314826965\n",
      "Epoch: 155, Train Loss: 0.15323880314826965\n",
      "Epoch: 156, Train Loss: 0.15323880314826965\n",
      "Epoch: 157, Train Loss: 0.15323880314826965\n",
      "Epoch: 158, Train Loss: 0.15323880314826965\n",
      "Epoch: 159, Train Loss: 0.15323880314826965\n",
      "Epoch: 160, Train Loss: 0.15323880314826965\n",
      "Epoch: 161, Train Loss: 0.15323880314826965\n",
      "Epoch: 162, Train Loss: 0.15323880314826965\n",
      "Epoch: 163, Train Loss: 0.15323880314826965\n",
      "Epoch: 164, Train Loss: 0.15323880314826965\n",
      "Epoch: 165, Train Loss: 0.15323880314826965\n",
      "Epoch: 166, Train Loss: 0.15323880314826965\n",
      "Epoch: 167, Train Loss: 0.15323880314826965\n",
      "Epoch: 168, Train Loss: 0.15323880314826965\n",
      "Epoch: 169, Train Loss: 0.15323880314826965\n",
      "Epoch: 170, Train Loss: 0.15323880314826965\n",
      "Epoch: 171, Train Loss: 0.15323880314826965\n",
      "Epoch: 172, Train Loss: 0.15323880314826965\n",
      "Epoch: 173, Train Loss: 0.15323880314826965\n",
      "Epoch: 174, Train Loss: 0.15323880314826965\n",
      "Epoch: 175, Train Loss: 0.15323880314826965\n",
      "Epoch: 176, Train Loss: 0.15323880314826965\n",
      "Epoch: 177, Train Loss: 0.15323880314826965\n",
      "Epoch: 178, Train Loss: 0.15323880314826965\n",
      "Epoch: 179, Train Loss: 0.15323880314826965\n",
      "Epoch: 180, Train Loss: 0.15323880314826965\n",
      "Epoch: 181, Train Loss: 0.15323880314826965\n",
      "Epoch: 182, Train Loss: 0.15323880314826965\n",
      "Epoch: 183, Train Loss: 0.15323880314826965\n",
      "Epoch: 184, Train Loss: 0.15323880314826965\n",
      "Epoch: 185, Train Loss: 0.15323880314826965\n",
      "Epoch: 186, Train Loss: 0.15323880314826965\n",
      "Epoch: 187, Train Loss: 0.15323880314826965\n",
      "Epoch: 188, Train Loss: 0.15323880314826965\n",
      "Epoch: 189, Train Loss: 0.15323880314826965\n",
      "Epoch: 190, Train Loss: 0.15323880314826965\n",
      "Epoch: 191, Train Loss: 0.15323880314826965\n",
      "Epoch: 192, Train Loss: 0.15323880314826965\n",
      "Epoch: 193, Train Loss: 0.15323880314826965\n",
      "Epoch: 194, Train Loss: 0.15323880314826965\n",
      "Epoch: 195, Train Loss: 0.15323880314826965\n",
      "Epoch: 196, Train Loss: 0.15323880314826965\n",
      "Epoch: 197, Train Loss: 0.15323880314826965\n",
      "Epoch: 198, Train Loss: 0.15323880314826965\n",
      "Epoch: 199, Train Loss: 0.15323880314826965\n",
      "Epoch: 200, Train Loss: 0.15323880314826965\n",
      "Epoch: 201, Train Loss: 0.15323880314826965\n",
      "Epoch: 202, Train Loss: 0.15323880314826965\n",
      "Epoch: 203, Train Loss: 0.15323880314826965\n",
      "Epoch: 204, Train Loss: 0.15323880314826965\n",
      "Epoch: 205, Train Loss: 0.15323880314826965\n",
      "Epoch: 206, Train Loss: 0.15323880314826965\n",
      "Epoch: 207, Train Loss: 0.15323880314826965\n",
      "Epoch: 208, Train Loss: 0.15323880314826965\n",
      "Epoch: 209, Train Loss: 0.15323880314826965\n",
      "Epoch: 210, Train Loss: 0.15323880314826965\n",
      "Epoch: 211, Train Loss: 0.15323880314826965\n",
      "Epoch: 212, Train Loss: 0.15323880314826965\n",
      "Epoch: 213, Train Loss: 0.15323880314826965\n",
      "Epoch: 214, Train Loss: 0.15323880314826965\n",
      "Epoch: 215, Train Loss: 0.15323880314826965\n",
      "Epoch: 216, Train Loss: 0.15323880314826965\n",
      "Epoch: 217, Train Loss: 0.15323880314826965\n",
      "Epoch: 218, Train Loss: 0.15323880314826965\n",
      "Epoch: 219, Train Loss: 0.15323880314826965\n",
      "Epoch: 220, Train Loss: 0.15323880314826965\n",
      "Epoch: 221, Train Loss: 0.15323880314826965\n",
      "Epoch: 222, Train Loss: 0.15323880314826965\n",
      "Epoch: 223, Train Loss: 0.15323880314826965\n",
      "Epoch: 224, Train Loss: 0.15323880314826965\n",
      "Epoch: 225, Train Loss: 0.15323880314826965\n",
      "Epoch: 226, Train Loss: 0.15323880314826965\n",
      "Epoch: 227, Train Loss: 0.15323880314826965\n",
      "Epoch: 228, Train Loss: 0.15323880314826965\n",
      "Epoch: 229, Train Loss: 0.15323880314826965\n",
      "Epoch: 230, Train Loss: 0.15323880314826965\n",
      "Epoch: 231, Train Loss: 0.15323880314826965\n",
      "Epoch: 232, Train Loss: 0.15323880314826965\n",
      "Epoch: 233, Train Loss: 0.15323880314826965\n",
      "Epoch: 234, Train Loss: 0.15323880314826965\n",
      "Epoch: 235, Train Loss: 0.15323880314826965\n",
      "Epoch: 236, Train Loss: 0.15323880314826965\n",
      "Epoch: 237, Train Loss: 0.15323880314826965\n",
      "Epoch: 238, Train Loss: 0.15323880314826965\n",
      "Epoch: 239, Train Loss: 0.15323880314826965\n",
      "Epoch: 240, Train Loss: 0.15323880314826965\n",
      "Epoch: 241, Train Loss: 0.15323880314826965\n",
      "Epoch: 242, Train Loss: 0.15323880314826965\n",
      "Epoch: 243, Train Loss: 0.15323880314826965\n",
      "Epoch: 244, Train Loss: 0.15323880314826965\n",
      "Epoch: 245, Train Loss: 0.15323880314826965\n",
      "Epoch: 246, Train Loss: 0.15323880314826965\n",
      "Epoch: 247, Train Loss: 0.15323880314826965\n",
      "Epoch: 248, Train Loss: 0.15323880314826965\n",
      "Epoch: 249, Train Loss: 0.15323880314826965\n",
      "Epoch: 250, Train Loss: 0.15323880314826965\n"
     ]
    }
   ],
   "source": [
    "### Autoregressive GRU\n",
    "class ProGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ProGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.PReLU()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.shape[0]\n",
    "        out, hn = self.lstm(x.view(batch_size, 1, -1), hidden)\n",
    "        out = self.activation(self.linear1(hn[0])) \n",
    "        out = self.activation(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out, hn\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros((1, 1, self.hidden_size), device=device)\n",
    "    \n",
    "# ProGRU Inputs\n",
    "input_size = 2 \n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "# Autoregressive Model\n",
    "model = ProGRU(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 6e-6)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 250\n",
    "                           \n",
    "avg_train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    losses_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        l, k = 0, 2\n",
    "        hidden = model.initHidden()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for j in range(4):\n",
    "            inp = X_train[i, l:k]\n",
    "            output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "            l += 2\n",
    "            k += 2\n",
    "        \n",
    "        loss = loss_fn(output.flatten(), y_train[i])        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_train.append(loss.detach().cpu().numpy())\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {np.average(losses_train)}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_train_loss.append(np.average(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f0a23b-eb2c-47a4-910a-e3a72406abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 500)\n",
    "\n",
    "y_pred, gt = [], []\n",
    "for i in range(len(x)):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        hidden = model.initHidden()\n",
    "\n",
    "        l, k = 0, 2\n",
    "        for j in range(4):\n",
    "            inp = X_test[i, l:k]\n",
    "            output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "            l += 2\n",
    "            k += 2\n",
    "\n",
    "            # print(f\"For x{j+1}, Pred y{j+1}: {output.item()}\\n\")\n",
    "\n",
    "        y_pred.append(output.item())\n",
    "        gt.append(y_test[i].item())\n",
    "        # print(f\"True y: {y_test[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe311c8-1c65-41ec-9b27-3c5351fcac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "plt.figure( figsize=(18, 12))\n",
    "plt.plot(x, gt, label=\"Ground Truth\", color=\"blue\")\n",
    "plt.plot(x, y_pred, label=\"Predictions\", color=\"orange\")\n",
    "plt.legend(loc=1 ,prop={'size': 12})\n",
    "plt.xlim(0, 502)\n",
    "# plt.ylim(-10, 10)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "# plt.savefig(\"GRU Predictions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37396d-f01d-47c2-a23d-c35823b50987",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Weights/weights_regressive_gru.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
